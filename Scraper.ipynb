{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Note Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from requests.exceptions import RequestException\n",
    "import time\n",
    "\n",
    "# Function to extract specific words from a given URL\n",
    "def extract_words_from_url(url):\n",
    "    try:\n",
    "        # Create a session object to persist settings across requests\n",
    "        session = requests.Session()\n",
    "\n",
    "        # TODO: Add retry logic to handle server errors (Hint: Use the Retry and HTTPAdapter classes)\n",
    "        retry_strategy = Retry(\n",
    "            total=3,\n",
    "            backoff_factor=1,\n",
    "            status_forcelist=[500, 502, 503, 504],\n",
    "            allowed_methods=[\"GET\"]\n",
    "        )\n",
    "        adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "        session.mount(\"https://\", adapter)\n",
    "        session.mount(\"http://\", adapter)\n",
    "\n",
    "        # TODO: Set up headers to mimic a browser (Hint: User-Agent and Accept headers)\n",
    "        headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\"\n",
    "        }\n",
    "        # Make the request to the URL (Hint: use session.get)\n",
    "        response = session.get(url, headers=headers)\n",
    "        # Make sure to raise an exception for any bad response\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Initialize an empty list to store extracted words\n",
    "        words = []\n",
    "\n",
    "        # TODO: Find all <span> elements with a specific class (Hint: use soup.find_all)\n",
    "\n",
    "        # TODO: Extract price range\n",
    "        price_range_element = soup.find('span', class_='y-css-1tsir1e')\n",
    "        if price_range_element:\n",
    "            price_range = price_range_element.text.strip()\n",
    "            words.append(price_range)  \n",
    "\n",
    "        # TODO: Extract review score\n",
    "        review_score_element = soup.find('span', class_='y-css-1jz061g')\n",
    "        if review_score_element:\n",
    "            review_score = review_score_element.text.strip() \n",
    "            words.append(review_score)  \n",
    "\n",
    "        # TODO: Extract category information\n",
    "        category_elements = soup.find_all('span', class_='y-css-1jz061g')  \n",
    "        categories = []  \n",
    "        for span in category_elements:\n",
    "            a_tag = span.find('a')  \n",
    "            if a_tag:\n",
    "                category = a_tag.text.strip()\n",
    "                categories.append(category)  \n",
    "        if categories:\n",
    "            words.append(\", \".join(categories))\n",
    "        \n",
    "        # TODO: Extract directions (address)\n",
    "        directions_element = soup.find('p', class_='y-css-jbomhy')\n",
    "        if directions_element:\n",
    "            address = directions_element.text.strip()  \n",
    "            words.append(address)  \n",
    "\n",
    "\n",
    "        # TODO: Extract opening hours\n",
    "        hours_table = soup.find('table', class_='hours-table__09f24__KR8wh')\n",
    "        if hours_table:\n",
    "            opening_hours = []\n",
    "            rows = hours_table.find('tbody').find_all('tr')\n",
    "            for row in rows:\n",
    "                day_element = row.find('th')\n",
    "                hours_element = row.find('td')\n",
    "        \n",
    "                if day_element and hours_element:  \n",
    "                    day = day_element.text.strip()  \n",
    "                    hours = hours_element.text.strip()  \n",
    "                    opening_hours.append(f\"{day}: {hours}\")  \n",
    "\n",
    "            if opening_hours:  \n",
    "                words.append(\", \".join(opening_hours))  \n",
    "\n",
    "        # Return the first 5 words or fewer if less\n",
    "        return words[:5]\n",
    "\n",
    "\n",
    "    except RequestException as e:\n",
    "        print(f\"Error scraping {url}: {e}\")  # Log the error message\n",
    "        return [\"\"] * 5  # Return a list of 5 empty strings if an error occurs\n",
    "\n",
    "    finally:\n",
    "        time.sleep(0)\n",
    "\n",
    "def scrape_urls_from_csv(input_csv, output_csv):\n",
    "    # Open the input CSV for reading and the output CSV for writing\n",
    "    with open(input_csv, mode='r', newline='', encoding='utf-8') as infile, \\\n",
    "         open(output_csv, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "\n",
    "        # TODO: Use csv.DictReader to read the CSV file and get a list of dictionaries, \n",
    "        # where each row is represented as a dictionary with column headers as keys.\n",
    "        reader = csv.DictReader(infile)\n",
    "\n",
    "        \n",
    "        # Get the fieldnames (i.e., the column headers from the input CSV)\n",
    "        fieldnames = reader.fieldnames\n",
    "\n",
    "        # TODO: Use csv.DictWriter to write rows into the output CSV file, \n",
    "        # making sure to include the same fieldnames as the input CSV.\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "\n",
    "        # Write the headers (column names) into the output CSV file\n",
    "        writer.writeheader()\n",
    "\n",
    "        # Iterate over each row in the input CSV\n",
    "        for row in reader:\n",
    "            # TODO: Extract the URL from the 'restaurant_url' column in the CSV.\n",
    "            url = row.get('restaurant_url')\n",
    "            # If the URL is missing, skip the row and print a message\n",
    "            if not url:\n",
    "                print(f\"Skipping row with missing URL: {row}\")\n",
    "                continue\n",
    "\n",
    "            # Print the URL being processed for debugging purposes\n",
    "            print(f\"Processing URL: {url}\")\n",
    "\n",
    "            # TODO: Call the extract_words_from_url function to get a list of words\n",
    "            extracted_words = extract_words_from_url(url)\n",
    "\n",
    "            # TODO: For each word in the extracted list, assign it to a new column (e.g., 'Label 1', 'Label 2', etc.)\n",
    "            # Ensure the columns are named 'Label 1', 'Label 2', etc., and each word is assigned to the correct column.\n",
    "            for i, word in enumerate(extracted_words):\n",
    "                row[f'Label {i + 1}'] = word\n",
    "\n",
    "            # Write the updated row into the output CSV\n",
    "            writer.writerow(row)\n",
    "\n",
    "            # TODO: Add a small delay between processing each URL to avoid overwhelming the server\n",
    "            # Hint: A delay of 1 second could be more appropriate\n",
    "            time.sleep(1)  # 1-second delay\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # TODO: Specify the input and output CSV file paths\n",
    "    input_csv = '/Users/julesvalois/Downloads/URLs to scrape.csv'  \n",
    "    output_csv = '/Users/julesvalois/Downloads/Scraped_results.csv'  \n",
    "\n",
    "    # Call the main scraping function\n",
    "    scrape_urls_from_csv(input_csv, output_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Details of the Code\n",
    "\n",
    "**Session with Retry Logic**:  \n",
    "This makes the code more robust by handling intermittent failures when accessing the URLs. It retries the request up to 5 times with increasing delays (exponential backoff).\n",
    "\n",
    "**Browser-Like Headers**:  \n",
    "The headers help disguise the script as a browser, preventing some websites from blocking it.\n",
    "\n",
    "**HTML Parsing with BeautifulSoup**:  \n",
    "The code searches for specific `<span>` elements with a defined class to locate words/keywords and extracts them.\n",
    "\n",
    "**CSV Handling**:  \n",
    "The input CSV is read row by row, URLs are processed, and extracted data is written back into a new CSV with additional columns (`Label 1` to `Label 5`).\n",
    "\n",
    "**Error Handling**:  \n",
    "If a request fails, the error is logged, and the script moves to the next URL, ensuring that an error doesn't stop the entire process.\n",
    "\n",
    "**Delay Between Requests**:  \n",
    "Although set to `0` for now, the `time.sleep(0)` can be adjusted to introduce a delay between requests to avoid overloading a server or getting IP blocked.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
